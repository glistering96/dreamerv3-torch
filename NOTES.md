# DreamerV3 TSP Implementation Notes

> **ëª©ì **: AI assistantê°€ ëŒ€í™” ì‹œì‘ ì‹œ ì´ íŒŒì¼ì„ ì½ê³  í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ë„ë¡ í•¨.
> **Repo**: NM512/dreamerv3-torch ê¸°ë°˜, TSP/CVRPìš©ìœ¼ë¡œ ìˆ˜ì •ë¨.

---

## Quick Reference

| Item | Value |
|------|-------|
| Best Return | -4.17 (optimal ~-3.8) |
| Mean Return | -7.7 |
| Key Fix | Advantage scaling (`adv / adv_scale`) |
| Main Issue | imag_mask accuracy ~50% |
| Verdict | ğŸŸ¡ ì—°êµ¬ìš© OK, Production ë¹„ì¶”ì²œ |

---

## 1. Architecture Overview

### Original vs Current

| Component | Original DreamerV3 | Current (TSP) |
|-----------|-------------------|---------------|
| Actor | MLP | TSPPointerActor (attention) |
| Heads | decoder, reward, cont | + mask, current_node, current_pos, coords |
| Imag reward | objective callback | Simulated distance |
| Mask | N/A | mask_simulation |

### Key Files

| File | Content |
|------|---------|
| `models.py` | WorldModel, ImagBehavior |
| `networks.py` | RSSM, TSPPointerActor |
| `envs/routing.py` | TSPEnv, CVRPEnv |
| `configs.yaml` | tsp_attn config |

---

## 2. Applied Fixes

### 2.1 Advantage Scaling (Critical)

**ë¬¸ì œ**: actor_grad_norm 0.03â†’0.009 vanishing  
**í•´ê²°**: `models.py`

```python
adv_scale = torch.clamp(adv.abs().mean(), min=0.1)
adv = adv / adv_scale
```

### 2.2 Hyperparameters

| Param | Before | After |
|-------|--------|-------|
| discount | 0.997 | 0.99 |
| imag_horizon | 15 | 10 |
| actor.entropy | 1e-4 | 1e-3 |
| actor.lr | 1e-4 | 3e-4 |

### 2.3 WM Reward Head (2026-01-08)

**ë¬¸ì œ**: Simulated distance reward ì‚¬ìš© â†’ WM reward head ë¯¸í™œìš©  
**í•´ê²°**: `models.py` Line 352

```python
# ì´ì „: reward = sim_reward
reward = reward_head(imag_feat).mode()
```

### 2.4 Raw Advantage in REINFORCE (2026-01-08)

**ë¬¸ì œ**: EMA normalized advantage â†’ Originalê³¼ ë‹¤ë¦„  
**í•´ê²°**: `models.py` Line 574-580

```python
raw_adv = target - self.value(imag_feat[:-1]).mode()
adv_scale = torch.clamp(raw_adv.abs().mean(), min=0.1)
raw_adv = raw_adv / adv_scale  # scalingì€ ìœ ì§€
```

**ìƒˆ ì‹¤í—˜**: `logdir_wmreward` (2026-01-08 23:00 ì‹œì‘)

---

## 3. Known Issues

### âš ï¸ Issue 1: Value Loss Double Target

```python
value_loss -= value.log_prob(slow_target.mode().detach())
```

â†’ Originalì—ë„ ë™ì¼, ì˜ë„ì  ì„¤ê³„ë¡œ ì¶”ì •

### âš ï¸ Issue 2: Imagination Mask Accuracy

`imag_mask_step_0 â‰ˆ 9.5/19` (50% ì •í™•ë„)  
â†’ World modelì´ ë°©ë¬¸ ìƒíƒœë¥¼ ì˜ ì˜ˆì¸¡ ëª»í•¨

### âš ï¸ Issue 3: REINFORCE Advantage

- Original: `target - value.mode()` (raw)
- Current: EMA normalized + scaled

---

## 4. Loss Audit Summary

| Loss | Distribution | Status |
|------|-------------|--------|
| coords | Normal | â“ |
| mask | Binary | âœ… |
| current_node | Categorical | â“ |
| current_pos | Normal | âœ… |
| reward | SymlogDisc | âœ… |
| KL | Dual KL | âœ… |
| Value | SymlogDisc | âš ï¸ |
| Actor | REINFORCE | âœ… |

### Loss í•„ìš”ì„± ë¶„ì„

| Loss | í•„ìˆ˜? | ì´ìœ  |
|------|-------|------|
| coords | â“ | ì—í”¼ì†Œë“œ ë‚´ ê³ ì •ê°’, ë§¤ë²ˆ ì˜ˆì¸¡ ë¶ˆí•„ìš” |
| current_node | â“ | current_posì™€ ì¤‘ë³µ (ë‘˜ ë‹¤ í˜„ì¬ ìœ„ì¹˜) |
| mask | âœ… | imaginationì—ì„œ ë°©ë¬¸ ì¶”ì ì— í•„ìˆ˜ |

---

## 5. Future Experiments

| ID | Experiment | Priority |
|----|-----------|----------|
| F1 | current_posë¥¼ ë…¸ë“œ ì¸ë±ìŠ¤ë¡œ ë³€ê²½ | Low |
| F2 | imag_gradient: dynamics í…ŒìŠ¤íŠ¸ | Medium |
| F3 | Curriculum (TSP-5â†’10â†’20) | Medium |
| F4 | objective callback ë³µì› | High |
| F5 | PPO + PointerNet baseline | High |
| F6 | coords_loss ì œê±° ì‹¤í—˜ | Medium |
| F7 | current_node_loss ì œê±° ì‹¤í—˜ | Medium |

---

## 6. Current Experiment Status (2026-01-11)

### Step 54,000 ğŸ”´ COLLAPSED

| Metric | ì´ˆê¸° | í˜„ì¬ |
|--------|------|------|
| Best | -4.31 | (ë³€í™”ì—†ìŒ) |
| **Last 50 Mean** | -7.6 | **-38.0** ğŸ’¥ |
| actor_grad_norm | 0.07 | **NaN** ğŸ’¥ |
| entropy | 0.1 | 3.0 (random) |
| value_min | -3.6 | **-178** ğŸ’¥ |
| target_min | -12 | **-179** ğŸ’¥ |

**Quartiles**: Q1=-8.2 â†’ Q4=**-38.0** (ì™„ì „ ë¶•ê´´)

### ê²°ë¡ 

í•™ìŠµ **ì™„ì „ ë¶•ê´´**. Value explosion ìˆ˜ì • ì—†ì´ ì§„í–‰ ë¶ˆê°€.

---

## 7. Improvement Plan

### ğŸ”´ ìš°ì„ ìˆœìœ„ 1: Imagination Mask ì •í™•ë„

**ë¬¸ì œ**: mask_step_0ì´ 50%ë§Œ ì •í™• â†’ imagination rolloutì´ ì˜ëª»ëœ ë§ˆìŠ¤í¬ë¡œ ì‹œì‘

**í•´ê²°ì±… ì˜µì…˜**:

- A) Ground truth mask ê°•ì œ ì£¼ì… (í˜„ì¬ë„ ì‹œë„ ì¤‘ì´ì§€ë§Œ ë¶ˆì™„ì „)
- B) Mask headì˜ loss weight ì¦ê°€
- C) Maskë¥¼ one-hotì´ ì•„ë‹Œ continuousë¡œ ë³€ê²½

### ğŸŸ¡ ìš°ì„ ìˆœìœ„ 2: Actor Exploration

**ë¬¸ì œ**: entropy 0.11ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ë ´ â†’ local optimumì— ê°‡í˜

**í•´ê²°ì±…**:

- actor.entropy coefficient ì¦ê°€ (1e-3 â†’ 5e-3)
- Initial random exploration steps ì¶”ê°€

### ğŸŸ¢ ìš°ì„ ìˆœìœ„ 3: Architecture ë‹¨ìˆœí™”

**ë¬¸ì œ**: ë¶ˆí•„ìš”í•œ prediction headê°€ í•™ìŠµ ë°©í•´ ê°€ëŠ¥

**í•´ê²°ì±…**:

- coords_loss ì œê±° (ì—í”¼ì†Œë“œ ë‚´ ê³ ì •ê°’)
- current_node_loss ì œê±° (current_posì™€ ì¤‘ë³µ)

---

## 8. Next Actions

1. [ ] value explosion ìˆ˜ì • (ì•„ë˜ ì˜µì…˜ ì¤‘ ì„ íƒ)
2. [ ] mask ground truth ì£¼ì… ë¡œì§ ê²€ì¦

---

## 9. ğŸš¨ Value Explosion Issue (2026-01-11)

### í˜„ìƒ

| Metric | ì´ˆê¸° | í˜„ì¬ |
|--------|------|------|
| value_min | -3.6 | **-130** |
| target_min | -12 | **-140** |
| value_std | 0.01 | **36** |

### ì›ì¸ ë¶„ì„ (ì•…ìˆœí™˜)

```
Mask 50% ë¶€ì •í™• â†’ invalid action ì„ íƒ â†’ penalty -2.0 ëˆ„ì 
     â†“
Value target -100+ â†’ Value network -130 í•™ìŠµ
     â†“
Bootstrap (value[-1] = -130) â†’ Lambda-return ì „íŒŒ
     â†“
target_min = -140 â†’ Value ë” ë‚˜ë¹ ì§ (ì•…ìˆœí™˜)
```

### ìˆ˜ì • ì˜µì…˜

| ì˜µì…˜ | ìˆ˜ì • íŒŒì¼ | ìˆ˜ì • ë‚´ìš© |
|------|----------|----------|
| **A** | `models.py:498-504` | imaginationì—ì„œ penalty ì œê±° |
| **B** | `models.py:530-535` | value target clipping ì¶”ê°€ |
| **C** | `models.py:437-444` | mask ground truth ê°•ì œ ì£¼ì… ìˆ˜ì • |

### ì˜µì…˜ A: Imagination Penalty ì œê±°

```python
# models.py:498-504
# ë³€ê²½ ì „: sim_reward += penalty
# ë³€ê²½ í›„: penalty ì¡°ê±´ ì‚­ì œ
```

### ì˜µì…˜ B: Value Target Clipping

```python
# models.py:530-535 ê·¼ì²˜
target = tools.lambda_return(...)
target = torch.clamp(target, min=-20, max=0)  # ì¶”ê°€
```

### ì˜µì…˜ C: Mask Ground Truth ê°•ì œ

```python
# models.py:437-444: start['mask'] ì‚¬ìš© í™•ì¸
# í˜„ì¬ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ - ë””ë²„ê¹… í•„ìš”
```

---

## 10. Root Cause Analysis (2026-01-11)

### ì›ë³¸ DreamerV3ì™€ ë¹„êµ ê²°ê³¼

| í•­ëª© | ì›ë³¸ | í˜„ì¬ (ë¬¸ì œ) |
|------|------|------------|
| imag_gradient | `dynamics` | `reinforce` |
| REINFORCE | raw advantage | adv_scale ë‚˜ëˆ—ì…ˆ â†’ NaN |

**ê²°ë¡ **: ì›ë³¸ì€ `dynamics` ëª¨ë“œê°€ ê¸°ë³¸ê°’. Actorê°€ world modelì„ í†µí•´ backprop.

### ì ìš©ëœ ìˆ˜ì •

- `configs.yaml`: `imag_gradient: dynamics` ë¡œ ë³€ê²½
- penalty: -2.0 â†’ -0.5
- target clipping ìœ ì§€

---

## 11. Mask Miss ì œê±° ì „ëµ

### í˜„ì¬ ìƒí™©

- miss_rate: 0.7%
- ì›ì¸: imagination ì¤‘ actorê°€ invalid action ì„ íƒ

### ì™œ 0%ê°€ í•„ìˆ˜ì¸ê°€?

- TSPì—ì„œ ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œ ì¬ë°©ë¬¸ì€ **ë…¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥**
- 0.7%ë„ ëˆ„ì ë˜ë©´ í•™ìŠµ ì‹ í˜¸ ì™œê³¡
- ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” maskê°€ ê°•ì œë˜ë¯€ë¡œ imaginationê³¼ ê´´ë¦¬ ë°œìƒ

### í•´ê²° ì˜µì…˜

#### A. Actor ì¶œë ¥ì— Hard Masking ê°•ì œ (ê¶Œì¥)

```python
# networks.py: TSPPointerActor
def forward(self, feat, mask=None, ...):
    logits = self.compute_logits(...)
    if mask is not None:
        logits = torch.where(mask, logits, torch.tensor(-1e9))  # â† ê°•ì œ
    return OneHotDist(logits)
```

- **íš¨ê³¼**: ì™„ì „íˆ 0% miss ë³´ì¥
- **ì£¼ì˜**: gradient flowì— ì˜í–¥ ì—†ìŒ (logitsë§Œ ì¡°ì •)

#### B. Masked Sampling êµ¬í˜„

```python
# action ìƒ˜í”Œ í›„ ê°•ì œ ë³´ì •
action = policy(...).sample()
if mask is not None:
    action = action * mask  # invalid action 0ìœ¼ë¡œ
    action = action / action.sum()  # renormalize
```

- **íš¨ê³¼**: ìƒ˜í”Œë§ ë ˆë²¨ì—ì„œ ë³´ì •
- **ì£¼ì˜**: gradient disconnection ê°€ëŠ¥

#### C. Penalty-Free Imagination

- Penalty ì œê±°í•˜ê³  mask missë¥¼ í—ˆìš©í•˜ë˜ ë¬´ì‹œ
- í•™ìŠµì— ì˜í–¥ ì—†ì´ ì§„í–‰
- **ë¹„ê¶Œì¥**: ê·¼ë³¸ í•´ê²° ì•„ë‹˜

### ê¶Œì¥ ìˆœì„œ

1. ~~**ì˜µì…˜ A êµ¬í˜„**~~ - ì´ë¯¸ êµ¬í˜„ë¨!
2. **ì‹¤ì œ ë¬¸ì œ ë°œê²¬** (ì•„ë˜ ì°¸ì¡°)

---

## 12. ğŸ”´ Root Cause: unimix_ratio (2026-01-11)

### ë°œê²¬

`TSPPointerActor.forward()`ì—ëŠ” ì´ë¯¸ hard masking ìˆìŒ:

```python
logits = logits.masked_fill(~mask, float('-inf'))  # âœ… ì ìš©ë¨
```

**í•˜ì§€ë§Œ!** `OneHotDist.__init__`ì—ì„œ:

```python
probs = probs * (1 - unimix_ratio) + unimix_ratio / num_actions
#                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
# ì´ê²Œ ëª¨ë“  actionì— 1% uniform í™•ë¥  ì¶”ê°€ â†’ maskedë„ í¬í•¨!
```

### í•´ê²°ì±… (í•œ ì¤„ ìˆ˜ì •)

```python
# networks.py Line 1069
# Before:
return tools.OneHotDist(logits, unimix_ratio=self._unimix_ratio)

# After:
return tools.OneHotDist(logits, unimix_ratio=self._unimix_ratio, mask=mask)
```

`OneHotDist`ì— ì´ë¯¸ `mask` íŒŒë¼ë¯¸í„°ê°€ ìˆê³ , unimix í›„ ì¬ì ìš© ë¡œì§ë„ ìˆìŒ!

---

*Last updated: 2026-01-11*
