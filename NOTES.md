# DreamerV3 TSP Implementation Notes

> **ëª©ì **: AI assistantê°€ ëŒ€í™” ì‹œì‘ ì‹œ ì´ íŒŒì¼ì„ ì½ê³  í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ë„ë¡ í•¨.
> **Repo**: NM512/dreamerv3-torch ê¸°ë°˜, TSP/CVRPìš©ìœ¼ë¡œ ìˆ˜ì •ë¨.

---

## Quick Reference

| Item | Value |
|------|-------|
| Best Return | -4.17 (optimal ~-3.8) |
| Mean Return | -7.7 |
| Key Fix | Advantage scaling (`adv / adv_scale`) |
| Main Issue | imag_mask accuracy ~50% |
| Verdict | ğŸŸ¡ ì—°êµ¬ìš© OK, Production ë¹„ì¶”ì²œ |

---

## 1. Architecture Overview

### Original vs Current

| Component | Original DreamerV3 | Current (TSP) |
|-----------|-------------------|---------------|
| Actor | MLP | TSPPointerActor (attention) |
| Heads | decoder, reward, cont | + mask, current_node, current_pos, coords |
| Imag reward | objective callback | Simulated distance |
| Mask | N/A | mask_simulation |

### Key Files

| File | Content |
|------|---------|
| `models.py` | WorldModel, ImagBehavior |
| `networks.py` | RSSM, TSPPointerActor |
| `envs/routing.py` | TSPEnv, CVRPEnv |
| `configs.yaml` | tsp_attn config |

---

## 2. Applied Fixes

### 2.1 Advantage Scaling (Critical)

**ë¬¸ì œ**: actor_grad_norm 0.03â†’0.009 vanishing  
**í•´ê²°**: `models.py`

```python
adv_scale = torch.clamp(adv.abs().mean(), min=0.1)
adv = adv / adv_scale
```

### 2.2 Hyperparameters

| Param | Before | After |
|-------|--------|-------|
| discount | 0.997 | 0.99 |
| imag_horizon | 15 | 10 |
| actor.entropy | 1e-4 | 1e-3 |
| actor.lr | 1e-4 | 3e-4 |

### 2.3 WM Reward Head (2026-01-08)

**ë¬¸ì œ**: Simulated distance reward ì‚¬ìš© â†’ WM reward head ë¯¸í™œìš©  
**í•´ê²°**: `models.py` Line 352

```python
# ì´ì „: reward = sim_reward
reward = reward_head(imag_feat).mode()
```

### 2.4 Raw Advantage in REINFORCE (2026-01-08)

**ë¬¸ì œ**: EMA normalized advantage â†’ Originalê³¼ ë‹¤ë¦„  
**í•´ê²°**: `models.py` Line 574-580

```python
raw_adv = target - self.value(imag_feat[:-1]).mode()
adv_scale = torch.clamp(raw_adv.abs().mean(), min=0.1)
raw_adv = raw_adv / adv_scale  # scalingì€ ìœ ì§€
```

**ìƒˆ ì‹¤í—˜**: `logdir_wmreward` (2026-01-08 23:00 ì‹œì‘)

---

## 3. Known Issues

### âš ï¸ Issue 1: Value Loss Double Target

```python
value_loss -= value.log_prob(slow_target.mode().detach())
```

â†’ Originalì—ë„ ë™ì¼, ì˜ë„ì  ì„¤ê³„ë¡œ ì¶”ì •

### âš ï¸ Issue 2: Imagination Mask Accuracy

`imag_mask_step_0 â‰ˆ 9.5/19` (50% ì •í™•ë„)  
â†’ World modelì´ ë°©ë¬¸ ìƒíƒœë¥¼ ì˜ ì˜ˆì¸¡ ëª»í•¨

### âš ï¸ Issue 3: REINFORCE Advantage

- Original: `target - value.mode()` (raw)
- Current: EMA normalized + scaled

---

## 4. Loss Audit Summary

| Loss | Distribution | Status |
|------|-------------|--------|
| coords | Normal | â“ |
| mask | Binary | âœ… |
| current_node | Categorical | â“ |
| current_pos | Normal | âœ… |
| reward | SymlogDisc | âœ… |
| KL | Dual KL | âœ… |
| Value | SymlogDisc | âš ï¸ |
| Actor | REINFORCE | âœ… |

### Loss í•„ìš”ì„± ë¶„ì„

| Loss | í•„ìˆ˜? | ì´ìœ  |
|------|-------|------|
| coords | â“ | ì—í”¼ì†Œë“œ ë‚´ ê³ ì •ê°’, ë§¤ë²ˆ ì˜ˆì¸¡ ë¶ˆí•„ìš” |
| current_node | â“ | current_posì™€ ì¤‘ë³µ (ë‘˜ ë‹¤ í˜„ì¬ ìœ„ì¹˜) |
| mask | âœ… | imaginationì—ì„œ ë°©ë¬¸ ì¶”ì ì— í•„ìˆ˜ |

---

## 5. Future Experiments

| ID | Experiment | Priority |
|----|-----------|----------|
| F1 | current_posë¥¼ ë…¸ë“œ ì¸ë±ìŠ¤ë¡œ ë³€ê²½ | Low |
| F2 | imag_gradient: dynamics í…ŒìŠ¤íŠ¸ | Medium |
| F3 | Curriculum (TSP-5â†’10â†’20) | Medium |
| F4 | objective callback ë³µì› | High |
| F5 | PPO + PointerNet baseline | High |
| F6 | coords_loss ì œê±° ì‹¤í—˜ | Medium |
| F7 | current_node_loss ì œê±° ì‹¤í—˜ | Medium |

---

## 6. Current Experiment Status (2026-01-11)

### Step 54,000 ğŸ”´ COLLAPSED

| Metric | ì´ˆê¸° | í˜„ì¬ |
|--------|------|------|
| Best | -4.31 | (ë³€í™”ì—†ìŒ) |
| **Last 50 Mean** | -7.6 | **-38.0** ğŸ’¥ |
| actor_grad_norm | 0.07 | **NaN** ğŸ’¥ |
| entropy | 0.1 | 3.0 (random) |
| value_min | -3.6 | **-178** ğŸ’¥ |
| target_min | -12 | **-179** ğŸ’¥ |

**Quartiles**: Q1=-8.2 â†’ Q4=**-38.0** (ì™„ì „ ë¶•ê´´)

### ê²°ë¡ 

í•™ìŠµ **ì™„ì „ ë¶•ê´´**. Value explosion ìˆ˜ì • ì—†ì´ ì§„í–‰ ë¶ˆê°€.

---

## 7. Improvement Plan

### ğŸ”´ ìš°ì„ ìˆœìœ„ 1: Imagination Mask ì •í™•ë„

**ë¬¸ì œ**: mask_step_0ì´ 50%ë§Œ ì •í™• â†’ imagination rolloutì´ ì˜ëª»ëœ ë§ˆìŠ¤í¬ë¡œ ì‹œì‘

**í•´ê²°ì±… ì˜µì…˜**:

- A) Ground truth mask ê°•ì œ ì£¼ì… (í˜„ì¬ë„ ì‹œë„ ì¤‘ì´ì§€ë§Œ ë¶ˆì™„ì „)
- B) Mask headì˜ loss weight ì¦ê°€
- C) Maskë¥¼ one-hotì´ ì•„ë‹Œ continuousë¡œ ë³€ê²½

### ğŸŸ¡ ìš°ì„ ìˆœìœ„ 2: Actor Exploration

**ë¬¸ì œ**: entropy 0.11ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ë ´ â†’ local optimumì— ê°‡í˜

**í•´ê²°ì±…**:

- actor.entropy coefficient ì¦ê°€ (1e-3 â†’ 5e-3)
- Initial random exploration steps ì¶”ê°€

### ğŸŸ¢ ìš°ì„ ìˆœìœ„ 3: Architecture ë‹¨ìˆœí™”

**ë¬¸ì œ**: ë¶ˆí•„ìš”í•œ prediction headê°€ í•™ìŠµ ë°©í•´ ê°€ëŠ¥

**í•´ê²°ì±…**:

- coords_loss ì œê±° (ì—í”¼ì†Œë“œ ë‚´ ê³ ì •ê°’)
- current_node_loss ì œê±° (current_posì™€ ì¤‘ë³µ)

---

## 8. Next Actions

1. [ ] value explosion ìˆ˜ì • (ì•„ë˜ ì˜µì…˜ ì¤‘ ì„ íƒ)
2. [ ] mask ground truth ì£¼ì… ë¡œì§ ê²€ì¦

---

## 9. ğŸš¨ Value Explosion Issue (2026-01-11)

### í˜„ìƒ

| Metric | ì´ˆê¸° | í˜„ì¬ |
|--------|------|------|
| value_min | -3.6 | **-130** |
| target_min | -12 | **-140** |
| value_std | 0.01 | **36** |

### ì›ì¸ ë¶„ì„ (ì•…ìˆœí™˜)

```
Mask 50% ë¶€ì •í™• â†’ invalid action ì„ íƒ â†’ penalty -2.0 ëˆ„ì 
     â†“
Value target -100+ â†’ Value network -130 í•™ìŠµ
     â†“
Bootstrap (value[-1] = -130) â†’ Lambda-return ì „íŒŒ
     â†“
target_min = -140 â†’ Value ë” ë‚˜ë¹ ì§ (ì•…ìˆœí™˜)
```

### ìˆ˜ì • ì˜µì…˜

| ì˜µì…˜ | ìˆ˜ì • íŒŒì¼ | ìˆ˜ì • ë‚´ìš© |
|------|----------|----------|
| **A** | `models.py:498-504` | imaginationì—ì„œ penalty ì œê±° |
| **B** | `models.py:530-535` | value target clipping ì¶”ê°€ |
| **C** | `models.py:437-444` | mask ground truth ê°•ì œ ì£¼ì… ìˆ˜ì • |

### ì˜µì…˜ A: Imagination Penalty ì œê±°

```python
# models.py:498-504
# ë³€ê²½ ì „: sim_reward += penalty
# ë³€ê²½ í›„: penalty ì¡°ê±´ ì‚­ì œ
```

### ì˜µì…˜ B: Value Target Clipping

```python
# models.py:530-535 ê·¼ì²˜
target = tools.lambda_return(...)
target = torch.clamp(target, min=-20, max=0)  # ì¶”ê°€
```

### ì˜µì…˜ C: Mask Ground Truth ê°•ì œ

```python
# models.py:437-444: start['mask'] ì‚¬ìš© í™•ì¸
# í˜„ì¬ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ - ë””ë²„ê¹… í•„ìš”
```

---

## 10. Root Cause Analysis (2026-01-11)

### ì›ë³¸ DreamerV3ì™€ ë¹„êµ ê²°ê³¼

| í•­ëª© | ì›ë³¸ | í˜„ì¬ (ë¬¸ì œ) |
|------|------|------------|
| imag_gradient | `dynamics` | `reinforce` |
| REINFORCE | raw advantage | adv_scale ë‚˜ëˆ—ì…ˆ â†’ NaN |

**ê²°ë¡ **: ì›ë³¸ì€ `dynamics` ëª¨ë“œê°€ ê¸°ë³¸ê°’. Actorê°€ world modelì„ í†µí•´ backprop.

### ì ìš©ëœ ìˆ˜ì •

- `configs.yaml`: `imag_gradient: dynamics` ë¡œ ë³€ê²½
- penalty: -2.0 â†’ -0.5
- target clipping ìœ ì§€

---

## 11. Mask Miss ì œê±° ì „ëµ

### í˜„ì¬ ìƒí™©

- miss_rate: 0.7%
- ì›ì¸: imagination ì¤‘ actorê°€ invalid action ì„ íƒ

### ì™œ 0%ê°€ í•„ìˆ˜ì¸ê°€?

- TSPì—ì„œ ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œ ì¬ë°©ë¬¸ì€ **ë…¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥**
- 0.7%ë„ ëˆ„ì ë˜ë©´ í•™ìŠµ ì‹ í˜¸ ì™œê³¡
- ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” maskê°€ ê°•ì œë˜ë¯€ë¡œ imaginationê³¼ ê´´ë¦¬ ë°œìƒ

### í•´ê²° ì˜µì…˜

#### A. Actor ì¶œë ¥ì— Hard Masking ê°•ì œ (ê¶Œì¥)

```python
# networks.py: TSPPointerActor
def forward(self, feat, mask=None, ...):
    logits = self.compute_logits(...)
    if mask is not None:
        logits = torch.where(mask, logits, torch.tensor(-1e9))  # â† ê°•ì œ
    return OneHotDist(logits)
```

- **íš¨ê³¼**: ì™„ì „íˆ 0% miss ë³´ì¥
- **ì£¼ì˜**: gradient flowì— ì˜í–¥ ì—†ìŒ (logitsë§Œ ì¡°ì •)

#### B. Masked Sampling êµ¬í˜„

```python
# action ìƒ˜í”Œ í›„ ê°•ì œ ë³´ì •
action = policy(...).sample()
if mask is not None:
    action = action * mask  # invalid action 0ìœ¼ë¡œ
    action = action / action.sum()  # renormalize
```

- **íš¨ê³¼**: ìƒ˜í”Œë§ ë ˆë²¨ì—ì„œ ë³´ì •
- **ì£¼ì˜**: gradient disconnection ê°€ëŠ¥

#### C. Penalty-Free Imagination

- Penalty ì œê±°í•˜ê³  mask missë¥¼ í—ˆìš©í•˜ë˜ ë¬´ì‹œ
- í•™ìŠµì— ì˜í–¥ ì—†ì´ ì§„í–‰
- **ë¹„ê¶Œì¥**: ê·¼ë³¸ í•´ê²° ì•„ë‹˜

### ê¶Œì¥ ìˆœì„œ

1. ~~**ì˜µì…˜ A êµ¬í˜„**~~ - ì´ë¯¸ êµ¬í˜„ë¨!
2. **ì‹¤ì œ ë¬¸ì œ ë°œê²¬** (ì•„ë˜ ì°¸ì¡°)

---

## 12. ğŸ”´ Root Cause: unimix_ratio (2026-01-11)

### ë°œê²¬

`TSPPointerActor.forward()`ì—ëŠ” ì´ë¯¸ hard masking ìˆìŒ:

```python
logits = logits.masked_fill(~mask, float('-inf'))  # âœ… ì ìš©ë¨
```

**í•˜ì§€ë§Œ!** `OneHotDist.__init__`ì—ì„œ:

```python
probs = probs * (1 - unimix_ratio) + unimix_ratio / num_actions
#                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
# ì´ê²Œ ëª¨ë“  actionì— 1% uniform í™•ë¥  ì¶”ê°€ â†’ maskedë„ í¬í•¨!
```

### í•´ê²°ì±… (í•œ ì¤„ ìˆ˜ì •)

```python
# networks.py Line 1069
# Before:
return tools.OneHotDist(logits, unimix_ratio=self._unimix_ratio)

# After:
return tools.OneHotDist(logits, unimix_ratio=self._unimix_ratio, mask=mask)
```

`OneHotDist`ì— ì´ë¯¸ `mask` íŒŒë¼ë¯¸í„°ê°€ ìˆê³ , unimix í›„ ì¬ì ìš© ë¡œì§ë„ ìˆìŒ!

### ê²€ì¦: ì˜í–¥ë°›ëŠ” Actorë“¤

| Actor | maskë¥¼ OneHotDistì— ì „ë‹¬? | ìˆ˜ì • í•„ìš”? |
|-------|-------------------------|-----------|
| **TSPPointerActor** (Line 1069) | âŒ | ğŸ”´ ìˆ˜ì • í•„ìš” |
| **MaskedActor** (Line 983) | âŒ | ğŸ”´ ìˆ˜ì • í•„ìš” |
| MLP.dist() (Line 749) | âœ… | âœ… ì •ìƒ |

**í˜„ì¬ ì‹¤í—˜**: `actor.type: tsp_pointer` â†’ **TSPPointerActor ì‚¬ìš© ì¤‘**

---

*Last updated: 2026-01-11*

## 13. ğŸ”´ Actor Gradient NaN Collapse (2026-01-13)

### í˜„ìƒ: Step 42,252ì—ì„œ í•™ìŠµ ë¶•ê´´

| Step | actor_grad_norm | actor_entropy | train_return |
|------|----------------|---------------|--------------|
| 10,000 | 0.08 | 0.12 | -7.3 |
| 20,000 | 0.14 | 0.05 | -7.2 |
| **42,152** | **0.14** | **0.038** | - (ë§ˆì§€ë§‰ ì •ìƒ) |
| **42,252** | **NaN** | 0.039 | - (ë¶•ê´´ ì‹œì‘) |
| 46,952 | NaN | **1.28** | -10.6 (random) |

### ì›ì¸

1. **Entropy 0.04ê¹Œì§€ ê°ì†Œ** â†’ ì •ì±…ì´ ê±°ì˜ deterministic
2. **OneHotDistì—ì„œ log(ê·¹ì†Œê°’) = -inf** â†’ gradient NaN
3. **Adam optimizer ìƒíƒœ ì˜¤ì—¼** â†’ í•™ìŠµ ì™„ì „ ì •ì§€
4. **Policyê°€ randomìœ¼ë¡œ íšŒê·€** (entropy 1.28)

### ì ìš©ëœ ìˆ˜ì • (2026-01-13)

#### 1. OneHotDist.log_prob() ì•ˆì •í™” (`tools.py`)
```python
def log_prob(self, value):
    logprob = super().log_prob(value)
    return torch.clamp(logprob, min=-20.0)  # -inf ë°©ì§€
```

#### 2. Entropy coefficient ì¦ê°€ (`configs.yaml`)
```yaml
actor:
  entropy: 1e-2  # 1e-3 â†’ 1e-2 (10ë°° ì¦ê°€)
```

#### 3. Gradient clipping ê°•í™” (`configs.yaml`)
```yaml
actor:
  grad_clip: 10  # 100 â†’ 10
```

---

## 14. ğŸ“Š í•™ìŠµ ë©”íŠ¸ë¦­ ë¶„ì„ (Step 0-42,000)

### Return ë¶„í¬

| Return Range | Count | Percentage |
|--------------|-------|------------|
| > -5 (near optimal) | 28 | 1.5% |
| -6 to -5 | 210 | 11.4% |
| -8 to -6 | 1,131 | **61.2%** |
| -10 to -8 | 431 | 23.3% |
| < -10 | 48 | 2.6% |

**ê´€ì°°**: ëŒ€ë¶€ë¶„(61%)ì´ -8 ~ -6 ë²”ìœ„ì— ì •ì²´. ë” ë‚˜ì€ í•´ë¡œ íƒˆì¶œ ëª»í•¨.

### Phaseë³„ ì„±ëŠ¥

| Phase | Mean Return | Best | Model Loss | Mask Loss |
|-------|-------------|------|------------|-----------|
| 0-5k | -21.3 | -5.6 | 14.8 | 8.7 |
| 5-10k | -7.4 | -4.5 | 7.0 | 2.9 |
| 10-20k | -7.3 | -4.4 | 4.5 | 1.6 |
| 20-30k | -7.4 | -4.7 | 4.0 | 1.5 |
| 30-42k | -7.2 | -4.5 | 4.0 | 1.4 |

**ê´€ì°°**: 5k step ì´í›„ mean returnì´ -7.xì—ì„œ ì •ì²´.

### ìˆ˜ë ´ ì†ë„

| Milestone | First Reached |
|-----------|---------------|
| return > -10 | Step 2,532 |
| return > -8 | Step 3,360 |
| return > -6 | Step 4,407 |
| return > -5 | Step 6,152 |
| return > -4.5 | Step 6,152 |

**Best Return**: -4.36 (Step 15,352)

---

## 15. ğŸš€ ìµœì í™” ê³„íš

### ë¬¸ì œ 1: Return -7.x ì •ì²´

**ì›ì¸**: 61%ê°€ -8~-6ì— ê°‡í˜, local optimumì—ì„œ íƒˆì¶œ ëª»í•¨

**í•´ê²°ì±…**:
- âœ… Entropy 1e-2ë¡œ ì¦ê°€ (exploration ìœ ì§€)
- ğŸ”² ë” ë†’ì´ í•„ìš”ì‹œ 3e-2ê¹Œì§€ ì‹œë„

### ë¬¸ì œ 2: ëŠë¦° ì´ˆê¸° í•™ìŠµ (5k stepsê¹Œì§€)

**ì›ì¸**: Prefillì´ random policyë¡œ ìˆ˜ì§‘ (mean -38.5)

**í•´ê²°ì±…**:
- ğŸ”² Prefill ì¤„ì´ê¸°: 2500 â†’ 1000
- ğŸ”² ë˜ëŠ” heuristic policyë¡œ prefill

### ë¬¸ì œ 3: ë‚®ì€ FPS (4.3 steps/sec)

**ì›ì¸**: í° ëª¨ë¸ + ë‹¨ì¼ í™˜ê²½

**í•´ê²°ì±…**:
- ğŸ”² `envs: 4`ë¡œ ë³‘ë ¬ í™˜ê²½ ì¦ê°€
- ğŸ”² `batch_size: 32`ë¡œ ì¦ê°€

### ë¬¸ì œ 4: Mask Loss ì •ì²´ (1.4-1.6)

**ì›ì¸**: World modelì´ ë°©ë¬¸ ìƒíƒœë¥¼ ì™„ë²½íˆ í•™ìŠµ ëª»í•¨

**í•´ê²°ì±…**:
- ğŸ”² mask_head loss_scale ì¦ê°€: 1.0 â†’ 2.0
- ğŸ”² ë˜ëŠ” coords_loss, current_node_loss ì œê±°

### ìš°ì„ ìˆœìœ„ ì‹¤í—˜ ìˆœì„œ

1. **A**: í˜„ì¬ ìˆ˜ì • (entropy 1e-2, grad_clip 10) í…ŒìŠ¤íŠ¸
2. **B**: envs 4, batch_size 32ë¡œ ì†ë„ ê°œì„ 
3. **C**: coords_loss ì œê±°, mask_head loss_scale 2.0
4. **D**: prefill 1000ìœ¼ë¡œ ê°ì†Œ

---

*Last updated: 2026-01-13*
